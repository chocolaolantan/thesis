% 自然言語処理について
\chapter{自然言語処理と文書要約}
本研究は文書要約を目的とする研究である．
本章では，要約のために必要な言語処理・文書処理などに関連する自然言語処理の技術について論じる．
また後半では，要約の基本的な考え方と関連する技術について論じる．


\section{言語解析について}
コンピュータによる言語処理には大きく分けて二つの処理が考えられる．
一つは入力として与えられた言語表現（文字列）を解析し，
その言語表現が表す意味内容をコンピュータの内部表現に変換する処理である\cite{book_ishizaki}\cite{book_takagi}\cite{book_yoshimura}\cite{book_nomura}．
通常，このような処理は言語解析（language\ analysis）あるいは，
言語理解（language\ understanding）と呼ばれる．
一方，コンピュータの内部表現を入力として，
それを適切に表す言語表現を作り出す処理は言語生成（language\ generation）と呼ばれる．
本章では，自然言語データからその意味内容をできるだけ損なわないように情報として
コンピュータに渡す自然言語解析について概説し，本研究との関連を示す．

\subsection{N-gram解析について}
言語の基本的な性質として古くから用いられてきたものは統計的な性質である．
言語の統計的性質のうちで最も簡単に調べられるものは，
文字の出現頻度であるが，対象としたテキストの種類にも大きく影響されるため，
出現頻度のデータはそういった意味であまり安定したものということはできない．

単独文字の出現頻度だけでなく，
２文字，３文字が隣接して生じる文字の共起関係の頻度を調べることによって
特徴を推定する試みがなされている．
これをそれぞれ，2-gram，3-gram，一般にN-gramと呼ばれている．
日本語についても同様の統計を取ることができるが，日本語には英語にない困難な問題がある．
それは英語圏で用いられるアルファベットが26文字であるのに対して，
日本語で用いられる文字は約4000文字もあるため，
2-gramの場合の表でも$4000\times 4000$という膨大なものになってしまう．
さらに困難なこととして，人名や地名には特殊な文字が使用されることがあるため，
漢字の種類を明確に固定することが難しいことが挙げられる．
よって，日本語については，言葉の基本となる文字の集合が必ずしも確定しているとは言い難く，
出現文字頻度を求めることは容易ではなく，信頼できる統計を得ることは困難である．
日本語の場合では4000文字以上あるので$n=3$としても$64$億の大きさになってしまう．
従って$n=3$程度以上の文字頻度統計をとることはあきらめられていた．
しかし，最近になって，大きなnに対しても文字頻度統計を取ることのできる方法は考案され，
nは一挙に255にまで拡大されている．

しかし，N-gram解析はN組単語の共起に関する単純な統計に基づいており，
言語の構造に関するモデルを陽にもたず，自然言語データの持つ情報の一部を解析しているに過ぎない．
そして言語解析では，単語の頻度統計を取る場合には単語とは何かということが問題となる．
複合語をどこまで認めるかや活用形を原型に直す部分に誤りがないかなど，
それは細かな点にまで及ぶ．

特に日本語の場合，英語のように単語がスペースで区切られてはいないので，日
本語テキストデータを自動的に単語に区切ることは大変難しい問題である．
この問題に関して本研究では形態素解析による対応を試みる．
形態素解析についての説明は次章で展開される．


\subsection{形態素解析について}
形態素解析は自然言語処理における第一段階の処理として重要な役割を担っている．
形態素解析の目的は，与えられた文を形態素・語の並びに分解し，
それぞれの品詞などを決定することである．
日本語文の形態素解析の難しさの第一の要因は入力文字列中の形態素あるいは語の区切りを認識する部分である．この入力文字列中のこの境界を同定する処理は一般にセグメンテーション(segmentation)と呼ばれている．
セグメンテーションを行った結果の各形態素に品詞を付与するのが形態素解析の大まかな流れである．
以下，これらの処理について概観する．

自然言語は以下に示すような階層的構造を持っている．
\begin{description}
  \item[音素]人間の意味（意志）伝達において音声をどのように扱っているかを基にとらえた音の単位．
  \item[形態素]意味を持つ最小の言語単位．一つ以上の音素からなる．
  \item[語]一つの意味のまとまりをなし，文法上一つの機能を持つ最小の言語単位．一つ以上の形態素からなる．
  \item[文節]文を意味上と発音上から不自然でない程度に区切った最小の言語単位．「（接頭辞）*（自立語）+（接尾辞または付属語）*」という構造を持つ．（*は０回以上の繰り返し，+は１回以上の繰り返し）例えば，「彼は」，「自然言語処理を」，「研究している」などのまとまりである．文節は，形態素解析において優先規則を適応する際に一つの基準として用いられたり，構文解析（次節参照）において処理の単位とされるなど，日本語解析において重要な役割を果たす言語単位である．
  \item[文]あるまとまった内容を持ち，形の上で完結した（表記において句点が与えられる）言語単位．一つ以上の単語からなる．
  \item[段落]文によるまとまった内容を介して，筆者の意図・主張として一つのまとまりを持つ．（表記の上では，改行されインデントされる）一つ以上の文からなる
  \item[文章]あるまとまった内容を表現するための文の順序づけられた集まり．隣接する文相互間にはある種の関係が存在する．
\end{description}

日本語の場合には語を区切る空白というものが存在しないため，
語の厳密な定義を与えることは困難である．
また，形態素という概念は欧米の言語学を発祥とするものであるため，
日本語における形態素の定義もそれほど明確ではない．
日本語における語は，文を構成する際の働きを基準とした場合，
まず自立語（content\ word）と付属語（function\ word）に大別され，
さらに細分化されて，一般には動詞や名詞などを含む10の品詞に分類される．

一方，語構成の立場から見ると，一つの要素からなる語と複数の要素からなる語がある．
複数の要素からなる語については以下の三つの結合形態がある．

\begin{description}
  \item[活用語]文中での働きの違いによって語形が変化する語．変化しない部分を活用語幹，変化する部分を活用語尾という．例えば「食べる」では「食べ（活用語幹）＋る（活用語尾）」となる
  \item[派生語]ある語に付加的要素が付いてできる語．もとの語を派生語幹，付加的要素を接辞（接頭辞または接尾辞）という．例えば，「寒さ」では「寒（派生語幹）＋さ（接尾辞）」，真冬では「真（接頭辞）＋冬（派生語幹）」となる．
  \item[複合語]複数の語が結合して１語となったもの．例えば「本＋棚」，「うれし＋涙」，「自然＋言語＋処理」などである．
\end{description}

日本語処理においては，上記の結合形態の各構成要素がほぼ英語の形態素に相当するものである．
しかし，接尾辞と助動詞の区別は明確にはなく，
またどのような基準で複合語を１語とみなすかも明確ではないため，
正確な形態素を獲得することは難しい．
だが，工学的に見たときの形態素解析の目的は，入力文を辞書中の項目の組み合わせに分解することであり，
形態素解析を後の言語処理のための第一段階の処理とする時には，
厳格な語，形態素の区別を必要としない立場もある．

このとき注意しなくてはならないことは，形態素レベルの曖昧性は後に続く処理の結果，
上手く解決することもあるが，その保障はなく，
むしろ曖昧性爆発の大きな原因になってしまう可能性を残す．
このことから，もっとも優先度の高い解，
あるいは上位ｎ個の優先解（これをn-best解と呼ぶ）が要求される場合が多い．

この解を求める際に利用される規則（情報）には大きく２種類あり，
一つは，いわば絶対的な規則で，それに違反するものは排除するという形で働き，
制約（constrain）と呼ばれる．
日本語の形態素解析では２語の連結可能性の規則などがこれにあたる．
もう一つの規則は，もっともらしい規則を優先するという形で働き，優先規則と呼ばれる．
このような優先規則をどのようにして計算機プログラムとして実現するかということが重要なタスクとなる．
以下で，各規則について述べる．

\subsubsection{２単語間の連接規則}
語の並びとして日本語文を見た場合，どのようなタイプ（品詞，活用形など）の２語が
連続して文中に現れるかという条件は明白である．
そこで，２語の連接可能性を規則（制約）として与え，
それを参照しながら入力文を語の並びに変換するという方法がもっとも基本的な形態素解析の方法である．

この処理は
\begin{enumerate}
  \item 辞書を参照して，入力文中の各位置から始まる語を取り出し，
  \item 連接可能性をチェックしながら取り出された語をつないでいく
\end{enumerate}

という二つの処理を繰り返し行うことによって実現される．
この処理によって得られる解析結果の例を図\ref{fig:clip001.eps}に示す．
このように解析結果は語をノードとするラティス構造(lattice\ structure)となる．
枝によって連結されている２語は連接可能だったことを示している．

% 図の挿入
\begin{figure}[tb]
  \begin{center}
    \includegraphics[keepaspectratio=true,height=40mm]{clip001.eps}
  \end{center}
  \caption{２単語間の連接規則による形態素解析の結果}
  \label{fig:clip001.eps}
\end{figure}

このような解析を行うために形態素解析では次の二つの辞書を用意する．

\begin{description}
  \item[単語辞書] 語の品詞，読み，活用などを指定する
  \item[連接可能性辞書] 連接可能な２語のタイプを指定する．語のタイプは，具体的な語であっても，品詞であっても，活用形であってもよい．文頭にあり得る語，文末にあり得る語は，それぞれ文頭，文末という特別なタイプと連接可能であるとする
\end{description}

連接可能性辞書は形態素解析が高速に行えるように，行は左側に現れる語のタイプ，
列は右側に現れる語のタイプとして，この２つのタイプが連接可能であれば，
成分を１に，そうでなければ０と表した行列の形に変換しておく．
こうすることによって，ある２語の連接可能性が，
左側の語の行番号と右側の語の列番号を参照するだけで調べられるようになる．



\subsubsection{優先規則：コスト導入}
図\ref{fig:clip001.eps}の解析結果からも分かるように連接可能性行列だけを用いた解析結果には不適切な解があまりにも多く含まれている．このように複数の解釈可能性が存在する時，この文は多義または曖昧性（ambiguity）を持つという．そこで，何らかの優先規則によって，もっともらしい解だけを選択することが必要となる．人工知能の分野では，経験的優先規則のことをヒューリスティックス（heuristics，発見的規則）と呼ぶ．日本語の単語分割では，最小一致及び分割数最小がヒューリスティックスとして古くから知られ，形態素解析に応用されている．

\begin{description}
  \item[最長一致法] 文字列の先頭から解析を始め，後続する可能性がある単語が複数あるときは，最長の単語を選択して先へ進む方法．長さの評価を２文節で行う２文節最長一致法もある．これは深さ優先探索なので，高速かつ使用する記憶領域が少ないが，先に短い語を取り出したほうが次により長い語と一致する時の可能性が考慮されていないため誤りも多い．
  \item[分割数最小法] 入力文字列を構成する文節の総数が最小になる解を優先する文節数最小方法と対象を文節ではなく，形態素数を最小とする解を優先する形態素数最小法がある．これは平均的な長さが最大である分割を求めることに相当し，総当たり探索なので，多くの記憶領域が必要である．また，分割数の同じ解釈が複数あった場合には，候補を選択するための別のヒューリスティックスが必要となる．
\end{description}

一般的に，分割数最小法の方が最長一致法よりも精度が高い．
これらの方法に共通する考え方は，できるだけ長い語によって構成される解，
あるいはできるだけ少ない語数の解を優先するというものである．
このような考えからに理論的な根拠は認められないが，
図\ref{fig:clip001.eps}の例でもそうであるように，妥当でない解のかなりの数部分が
不必要に短い語を含むものであることから，直感的にはその良さを認めることができる．
しかし一般に，品詞の接続を可能か不可能かという二値で評価することは難しく，
接続の強弱を表す枠組みが必要であり，接続コスト最小法が知られている．
\begin{description}
  \item[接続コスト最小法] 文の単語分割に対して何らかの接続コストを設定し，文全体で接続コストの和が最小になるように単語分割を選択する方法を接続コスト最小法という．接続コスト最小法では，品詞接続コストと単語コストを定義する．品詞接続コストは接続が希な品詞間ほど大きく，単語コストは出現頻度が小さいほど大きくなるように，設定する．ただし，品詞接続コストと単語コストの具体的な値は人手にとり決定する．
\end{description}

接続コスト最小法は，最長一致法や分割数最小法に比べて計算量は多いが，
解析精度は高いので，近年では仮名漢字変換アルゴリズムの主流にもなっている．
また，日本語形態素解析のフリーソフトとして有名な
JUMANや茶筅\cite{chasen}も接続コスト最小法を用いている．
以下，本研究で用いた日本語形態素解析システム茶筅の概要について説明する．



\subsubsection{形態素解析システム『茶筅』}
茶筅は，1992年に公開された日本語（Japanese）の，
利用者によるカスタマイズ可能な（User-Customizable）形態素解析（Morphological\ Analyzer）
という設定で，当時京都大学の長尾研究室で開発されたJUMAN\cite{juman}の後継機である．
JUMANは論理プログラミング言語を用いて動的計画法をバックトラックなしに並行探索する手法を用い，
品詞セットの定義や，活用のタイプや具体的な活用形など辞書に関する全ての項目を利用者が
自由に定義し記述できることを基本方針とした形態素解析システムである．
茶筅はこれに，文字列検索のためにパトリシア木などの技術を取り入れ，
JUMANの辞書システムを改良し，辞書のコンパイル時間が約$1/5$，
解析速度は10倍以上の高速化を実現している．

JUMANへの追加機能としては，連接表を自動学習するところである．
コスト最小化の考え方は，確率最大化という考え方と等価であるとし，
音声認識で定着していた隠れマルコフモデル（HMM）の考え方を取り入れた．
小規模な品詞タグ付きデータからの初期値をとれば，
大量の未解析データについてHMMを適応することにより，
ある程度の性能を達成できることがわかっていたが，扱っていた品詞のセットは巨大であり，
未知の現象については，やはり正しい事例を蓄積する必要性があった．
また連接コスト表は，基本的に連続する二つの品詞（または単語）の関係であり，
HMMでいえば2-gramのモデルであったので，全ての現象について二つを超える品詞列を見るには，
極めて大量の学習データを必要とした．
加えて，一部の現象については，二つの品詞連続を見ていただけでは
正しい解析を行えないという課題もあったが，そのころ，可変長のマルコフモデルという，
2-gram，3-gramさらにそれ以上の長さを併用したモデルを用い，
連接表を状態遷移と考えることで無理なく実現できることを利用した．
これにより可変長の連接規則を状態遷移表にコンパイルすることで実行効率を落とすことなく，
可変長の連接規則を扱うことができるようになっている．

\subsection{構文解析について}
構文解析では，形態素解析によって品詞を付与された形態素あるいは語の列を句や節にまとめ，
それらの間の文法的な関係を同定し，その構造を明らかにすることである．
文の構造の多くの場合，意味的・文脈的情報がなければ一意に決定することはできない．
形態素解析同様，文法規則（制約）および種々の優先規則によって入力文を解析し，
最も優先度の高い解を求めることとなる．

この解析の結果としてどのような構造が得られるかは，解析に用いる文法の枠組みに依存するが，
一般的には句構造（phrase\ structure）や依存構造（dependency\ structure）を利用することが多い．
図\ref{fig:clip002.eps}は，"先生は自転車で学校に行った"という文を解析して得られる句構造の例である．
句構造は，文のどの部分が一つの構成要素にまとめられるかを表す
階層的な構成素構造（constituent\ structure）と，
そのまとまりが属する（grammatical\ category）の２つの情報を表現し，
一般には木構造あるいは括弧付けによって表現する．

\begin{figure}[tb]
  \begin{center}
    \includegraphics[keepaspectratio=true,height=60mm]{clip002.eps}
  \end{center}
  \caption{句構造の例}
  \label{fig:clip002.eps}
\end{figure}


句構造が文の構成的な側面に注目しているのに対し，
依存構造では修飾語と被修飾語の関係を重視して文の構造を表す．修飾（依存）関係は有効辺で表し，
辺の元が辺の先を修飾していることを表す．
これは日本語における「係り受け」と呼ばれる関係に相当する．
構文解析に用いる情報には，語の人称，性，数に関する情報や語形変化に関する情報などがある．
ただし，これらの中には必ずしも辞書に記述する必要がないものもある．
名詞の単数複数，活用形の変化系を全て辞書項目として辞書に登録すると登録語数は増大してしまう．
不規則変化をする語は辞書に登録せざるを得ないが，
規則的な変化で表現される情報については語の原型のみを登録し，
形態素解析によって解析時に動的に抽出するのが普通である．
この場合，変化系を品詞の種類として扱うか，品詞とは独立の文法素性として扱うかの選択があり得る．

その他に，構文解析で用いる情報として代表的なものに
下位範疇化情報（subcategorize\ information）と呼ばれるものがある．
これは語がどのような語を格要素（case element）として取り得るかを記述したものである．
格要素とは，日本語の場合では格助詞に対応したガ格，ヲ格などの格関係を介して，
語が別の語と依存関係を持つ場合，依存する語のことを指す．
下位範疇化情報の例として，情報処理振興事業協会センター（IPA）で開発された
IPAL動詞辞書（情報処理進行事業協会技術センター，1987）がある．
IPAL動詞辞書には日本語の和語動詞861語を対象として，言語学的考察に基づくコンピュータ利用を前提として記述されたものである．

当初の研究では，二文節間の係りやすさを決定するルールを人手で作成されていたが，
係り受け解析で用いられる素性集合は膨大で，それらは競合することが多いため，
網羅性，一貫性という観点から問題が多い．
近年では，構文情報が付与された大規模コーパスの利用が可能になってきており，
機械学習アルゴリズムを用いた統計的な構文解析技術が提案されるようになった．
その中で，統計的係り受け解析は，文中の任意の二文節間の係りやすさを数値化した行列を作り，
その中から動的計画法を用いて文全体を最適にする係り受け関係を求めるというモデルに
基づくものが主流であった．
しかしながら，解析時や学習時に全ての係り受け関係の候補を対象にしなければならないために，
効率が良いとは言えなかった．
さらに各二文節の係り受け関係は他の関係との独立を仮定しているが，
ある係り受け関係が他の係り受け関係に影響を及ぼすこともあり，この仮定は必ずしも適切ではない．
このような問題に対してチャンキングの段階適応による係り受け解析モデルを利用し，
機械学習アルゴリズムにSVM（Support\ Vector\ Machine）を採用した
南瓜\cite{kudoh}という構文解析システムが提案されている．


\subsection{意味解析について}
意味とは，ある表現形式によって示される内容であり，
自然言語における意味的なまとまりとして，
語，句，節，文，文の集合などのいくつかのレベルが考えられる．
従来の自然言語理解の研究では，これらいくつかのレベルの内容を適切に表現できる形式が模索され，
論理式，意味ネットワーク，概念依存構造などの意味構造が研究されてきた．
語は意味構造の基本的な構成要素として位置づけることができ，語の意味は，語義として定義される．
しかし，その設定にあたっては，他の語との関係や他の言語との対応関係といった言語学的な観点や，
機械翻訳などの工学的な観点からも研究されていて，重要なテーマの一つである．
基本的に意味構造は，語の意味と語や句や節などのまとまりの間に表現される．
単純なモデルにおける文の意味構造は，
文に存在する単語の意味と単語間の意味関係の２種類によって表現される．
そうでない場合は，領域知識や常識といったような背景知識がなければ，
正確には表現できない．
意味解析とは，与えられた文から妥当な語の意味と意味関係を明らかにし，
意味構造を生成することといえる\cite{oishi}．


\subsection{文脈解析について}
文脈解析は，文と文との間の関係を捉えることにより，
テキストによって表されている概念や事象のつながりを明らかにすることを目的とする．
筆者が伝えたい内容は，複雑に絡み合っていたり，時間的なつながりがあったりする．
だが，それを伝達する手段であるテキストは一次的な表現であるため，
全ての情報を一度に伝えることはできない．
従って，読者は，その伝えられてきたテキストから概念や事象のつながりを復元することが必要である．
伝えられるテキストは，聞き手がそれまでの話の内容や外界の状況を理解していること，
また，言語の知識や世界に関する知識（常識）を共有していることを前提に展開される．
したがって，計算機による文脈解析では，そのような言語や常識に関する知識を用い，
代名詞や指示詞などを含めたテキスト中の各語が指している概念を明らかにすることや，
文間の論理的なつながりを明らかにすることが行われる．


\subsubsection{結束性}
筆者が読者に伝えたい概念は空間的な広がりを持っていたり，
時間的につながりを持つものであったり，過去に伝えた概念と同じものであったりするなど，
様々な概念を持つ．このような概念や事象のつながりを結束性という．
テキストの結束性はネットワークを用いて表現することができる（図\ref{fig:clip016.eps}）．
このときノードは，単語であり，リンクは単語による概念や事象間の意味的な関係を表している．

% 図の挿入
\begin{figure}[htbp]
  \begin{center}
    \includegraphics[keepaspectratio=true,height=50mm]{clip016.eps}
  \end{center}
  \caption{テキストのグラフトポロジー}%{}内にタイトルを記入してください
  \label{fig:clip016.eps}
\end{figure}


\subsubsection{照応関係}
ある言語表現が，後に現れる言語表現と同じ内容は対象を指すとき，
これらの表現は照応関係にあるという．
このとき，前者の表現を先行詞，後者の表現を照応詞という．
照応関係を明らかにするには，まず係り受けなどの構文解析を行って文の基本的な構造を明らかにし，
また係助詞「は」のついた名詞の解析や，「が」格などの表層格の解析などを行って，
文の主題・焦点などを明らかにしておかなければならない．
テキスト情報の中で未決定の照応詞の候補を見つけると，
それに対応する先行詞があるかどうかを調べる照応解析に入る．
まず，先行詞になりそうな主題や焦点，名詞や用言などを列挙する．
次に，これらのうちどれが照応詞に対する先行詞として最も妥当であるかを決定する必要があるが，
言語理論の体場からはまだ明確で確定的な方法は作られていない．
そこで人工知能の研究分野で買いたくされたエキスパートシステムの考え方で
試行錯誤的な方法を取らざるを得ない．
すなわち，照応関係にはテキスト中の種々の表現が複雑に関わっているので，
ある種の表現が存在する時には，ある種の先行詞を指す可能性が高いといった経験則（推論規則）が
用いられている．

一つの照応関係の決定にただ一つの経験則が適応可能という場合は稀で，
いくつかの経験則が同時に適応されることが多いので，
それぞれの経験則に確信度を付与しておいて，経験則が適応されるたびにその確信度を足し合わせていき，
最終的に最も高い確信度を与える候補を先行詞として，決定する過程を取る．
この場合のシステム全体の流れを図\ref{fig:clip003.eps}に示す．

% 図の挿入
\begin{figure}[htb]
  \begin{center}
    \includegraphics[keepaspectratio=true,height=30mm]{clip003.eps}
  \end{center}
  \caption{照応解析の解決手順}
  \label{fig:clip003.eps}
\end{figure}


基本的にはこの方法は，定名詞，指示詞，代名詞，ゼロ代名詞
\footnote{すでに，読者の意識にのぼっていて，容易に補完できると判断される場合には，
照応詞が表現されないことがあり，ゼロ代名詞と呼ばれる．}のそれぞれの先行詞の
推定に共通するものであるが，用いる手がかりや，照応詞の性質の違いにより，細かい手順や，規則は異なる．

% 自然言語処理長くなったから改ページ
\newpage

\section{基本的な要約手法}
一般に要約とは，筆者が主張する話題を中心に，原文の大意を保持したまま，
テキストの長さ，複雑さを減らす処理である．要約は，その機能の観点から四つに分類することができる．

\begin{enumerate}
  \item 判断材料としての要約
  \item 内容情報を提供する要約
  \item 評価を含む要約
  \item 比較を含む要約
\end{enumerate}

(1)の機能は，読者にそのテキストを読むかどうかを判断する情報を与える機能である．
読者は内容を完全に把握できなくても，
そのテキストが自分の現在の関心に関係があるかどうかを判断できればよいとする立場である．
テキストが自分の関心に関係があるかを判断する基準にタイトル情報なども考えることができるが，
チェーンメールやスパムメールに代表されるように，
本文を開かせるためにわざと抽象的なタイトルをつけることも考えられる．
また，タイトルが内容を考慮してつけられたものかどうかの確証がない場合には，
タイトルのみの情報では不十分である．
このような状況では，本文から抽出された重要な文を見て，
文脈は不完全であっても，チェーンメールか否かの判断をする際の材料になることは可能であるだろう．

(2)の機能はテキストの内容を正確に伝える機能である．
(1)よりも洗練された要約文が要求されるが，入力テキストのみの情報で閉じていることが特徴である．
如何にして入力テキストから高品質な要約を獲得するかに主眼が置かれている．

(3)の機能は，テキストの内容に加えて，その内容を評価する情報も伝える．
よってテキストに対する価値基準となる知識もシステムは持たなければならない．

(4)の機能は，そのテキストのみならず，関連するテキストの内容も含めて，
その話題に関するテキストの内容も提供する．
ゆえにシステムは要約データベースを持たねばならず，出力として得られる要約は，
その話題と関連する話題に関する一種の概説に相当する．

これらの機能は包含関係にあり，(1)の機能は(2)の機能によって実現可能であるし，
同様にして(4)の機能を有するシステムであれば，全ての機能を実現できることになる．
一般に「要約」という場合は(2)の機能を有することが期待される．

要約生成は，テキストの内容を理解し，中心的な話題を特定し，
それを簡潔にまとめるという三つの作業からなる．
テキストの内容の理解には，高度な言語処理が要求され，
これらの解析を行ったとしても，現段階において，計算機がテキストを正確に理解できるとは言えず，
話題の特定や簡潔な文生成という言語処理も困難が多い．
よって計算機による要約の第一段階としては，
重要な情報を伝えている文をテキスト中から抜き出すことによって実現される場合がほとんどである．
要約に要求される必要レベルは利用用途によって様々である．
読み手が内容を完全に理解できていなくても，
そのテキストが自分の現在の関心に関係があるかどうかの判断材料として用いる要約や，
テキストの分類を考える際に，テキストの全文を走査するコストを削減するための要約では，
抄録の範疇でも十分に対応できる．
よって抄録作成においては，どれだけ高品質で情報量の多い文を抽出できるかに焦点が絞られる．
以下では，これまでに提案された重要文の抽出手法を概説する．

続けて，
\begin{itemize}
  \item 抽出した文中に代名詞などが含まれている場合，その先行詞が要約文中に存在する保障がない
  \item テキスト中の様々な箇所から抽出してくるため，抽出した複数の文間のつながり（首尾一貫性）がない
\end{itemize}
のような抄録の問題に対しての，これまでの照応詞補完や文脈解析などの言語処理の対応による研究を概説する．高品質な要約の獲得のためには，原文の内容の知識・常識といった知識データの必要性やユーザの情報要求の反映といった技術が関わってくる．次小節では，これら要約の精度を上げる研究に説明する．最後に困難な要約の評価についての研究をまとめる．

一般的に要約とは，筆者が主張する話題を中心と考え，
その中心にしたがって原文の大意を保持したままテキストの長さ，
複雑さを減らす処理である．要約の作成はテキストの内容を理解し，
中心的な話題を特定し，それを簡潔にまとめるという三つの作業からなる．
テキストの内容の理解には，意味解析，文脈解析といった高度な言語処理が
要求され，これらの解析を行なったとしても，現在の段階では
計算機がテキストを性格に理解できているとはいえない．
また，話題の特定や簡潔な文生成という言語処理も困難が多い．

そのような状況ではあるが，
要約の基本的な手法は，言語空間レベルでとらえることが出来，
2つの以下の広い手法が同定できる．

\subsection{表層的手法}
さまざまな要素が異なったレベルで表現されているが，
構文レベルの表現を超えようとはしない．たとえば，
要約作成にあたり単語は意味レベルまで解析されるが，
文は構文レベルまでしか解析されないということもある．
この手法は文書から重要文を抽出することで抜粋を作成する．
もちろん，文は文脈を無視して抽出されるかもしれない．
この場合，合成の段階で，抽出による一貫性のなさを修復するために
出力された抜粋を補正したり，よりコンパクトにするため，
テキストの再構成をしたりする．しかし，その様な補正には，
正確な照応解決を含む多くの問題がある．
一般的に照応解決をコンピューターで行なうのは難しいからである．

この表層的手法は，原文の顕現的な部分を抽出し，
これらを効果的な方法で並べて提示することにとどめた
要約手法であるといえる．
本研究で考える手法も「表層的手法」といえるであろう．

\subsection{深層的手法}
文の意味レベルの表現を仮定する．意味あるいは談話レベルの表現から
の自然言語作成が行なわれ，その結果，原文にない部分を含む要約である
アブストラクトを要約として出力する．
この深層的手法を考慮したシステムは，抽出した談話をどのように結びつけるか
に関するさまざまな規則を適用することで，一貫した要約の作成を目指すことが可能になる．
意味解析と合成の構成素はかなり知識集約的なので，特定の領域を仮定する
ことが必要となる．
通常，一般目的の知識ベースは手に入らないか，入ったとしてもうまくいかない．
この手法の多くは，入力として構造化データを用いる．
例えば，バスケットボールの統計表からシーズンの成績の要約を，
株価表から株価報告を生成する．深層的手法といえる他の手法は，
ある要素にしか意味解析を用いない．
例えば，シソーラスを単語の一般化に用いる．
しかし，深層的手法はより情報量のある要約の作成を約束する．

\section{重要文抽出による要約}
\label{critical_sentence}
本研究では重要文を抽出し抜粋を作成することで，原文に対する要約と考えている．
そのような要約を作成する手法は，主として重要な文をどのようにして
同定するかによって分類できる．
具体的には．何らかの情報を利用して各文に得点を付け，
最も得点の高いものから抜き出し要約として出力することになる．
このようなタイプの要約を作成する方法を文の得点を計算する情報に基づいて分類すると，
以下のようになる\cite{text_autosum}．

\begin{enumerate}
  \item テキスト中のキーワードの出現頻度を利用する．
  \item テキスト中あるいは段落中での位置情報を利用する．
  \item テキストのタイトル情報を利用
  \item テキスト中の手がかり表現を利用する．
  \item テキスト中の文間の類似性の情報を利用する．
  \item テキストの文中あるいは単語間のつながりの情報を利用する．%ここに項目を書く
\end{enumerate}

一般には上の1〜6によって，求められた単語の重要度を$w_i$を計算し，
式(\ref{weight_equation})によって文の重要度$S_j$を求め，
重要度の高い順番から要約率に対応した文の数（行）だけ要約として出力することとなる．

\begin{equation}
\label{weight_equation}
S_j=\sum_{i}w_i
\end{equation}

以下，重要度に用いる情報と，それぞれの情報との本研究とのかかわりについて概説する．

\subsection{単語の出現頻度の利用}

テキストの話題（主題）を示す傾向のある重要語はテキスト中においてよく出現する
という仮定に基づき，「テキスト内で頻繁に出現している語は重要である」と考えるTF法
（Term\ Frequency：式(\ref{weight_tf})），
\begin{equation}
\label{weight_tf}
w_{tf}=テキスト内の単語iの出現回数
\end{equation}

「抽象度が低いほど少数のテキストに出現し，
またそのような語を重要である」と考えるIDF法(Inverse\ Document\ Frequency：
式(\ref{weight_idf}))，

\begin{equation}
\label{weight_idf}
w_{idf}=\log \frac{単語iが出現したテキスト数}{総テキスト数}
\end{equation}

これらTF法IDF法を組み合わせることで
テキスト固有の出現割合を計算するTF$\cdot$ IDF法（式(\ref{weight_tfidf})）
などさまざまな単語重み付け技法がある．

\begin{equation}
\label{weight_tfidf}
w_{tf\cdot idf}=w_{tf}\times w_{idf}
\end{equation}

このような単語の出現頻度に基づく重み付けを元に文の重要度を決め，
その文の重要度を元にした重要文を抽出法が1950年代から提案されている．

本研究でも単語の重みを単語の出現頻度に基づいて単語の重要度を求め，
その重要単語に基づいて文の重要度を算出している．


\subsection{テキスト中での位置情報}
テキストは，ジャンルに依存してある程度構造的に規則性があると考えられている．
例えば，学術論文では，序論，本論，結論のような構造を持ち，新聞は，見出し，
小見出しの後に本文は結論から展開されることが多い，
このようにテキスト構造から，テキスト中での重要な文の重要度計算に
利用する手法がいくつか考えられる．
具体的には，論説文の場合では，テキスト全体のまとめは書き出しや結び近くにあると考えられ，
重要な文は段落の先頭や最後，節の見出しの直後に述べられていると推察される．
また新聞記事の場合であると，本文の先頭から数文を抽出するのが良いとされている．
この手法は，lead法と呼ばれており，新聞では結論や事実をまず最初に伝え，
そこから状況説明がなどが展開されることが多いという
新聞記事の構造に基づいた位置情報の利用があげられる．

% 図の挿入
\begin{figure}[h]
  \begin{center}
    \includegraphics[keepaspectratio=true,height=100mm]{clip017.eps}
  \end{center}
  \caption{Yahooのニュース記事}%{}内にタイトルを記入してください
  \label{fig:clip017.eps}
\end{figure}

新聞と似たような例であるが，Yahoo\footnote{http://www.yahoo.co.jp/}というポータルサイト内の
ニュース記事にもlead法の考えが用いられている．
読者に対して，始めから記事全文を出力するのではなく，
Fig.\ref{fig:clip017.eps}のようにタイトルと始めのパラグラフのみを出力し，
そのテキストから閲覧者は記事を読む判断材料を得るのである．


\subsection{タイトル情報の利用}
ジャンルにより決まったテキスト情報から得られる情報として，
テキストに付与されたタイトル，見出しの情報がある．
例えば，学術論文の場合では，テキスト全体にタイトルが与えられ，
また，各章，各節にも見出しが付与されることが多い．
このタイトルや見出しは，ある意味でテキスト本文の非常に簡潔な
要約であるとも考えられる．そのため，タイトルや見出しに現れる
内容語を含む文が重要であると考え，重要文抽出に利用する方法が
いくつか提案されている．

本研究ではタイトルの情報を文の重要度に反映させるような
システムを考えてはいない．
判例文にはその訴訟事件の内容と裁判所の判断以外に
事件を考察するにあたり用いられた参考文献の記述がある．
その参考文献の一つに参照条文という記述があるが，
文の重み付けには参照した条文の情報を文の重要度に
反映させている．

\subsection{手がかり表現}
文書の中にはテキストや文の主題を表す内容語ではないが，
テキスト中の重要箇所を指示すると考えられる手がかり表現
や手がかり語がいくつか存在する．
例えば，論文では「結論として，」や「本研究では，」などが挙げられる．
これらの表現はテキストの分野に依存し特許申請文書であったりすると
「本題」や「〜に関する」といったものが重要な手がかり語や表現であるといえる．

また，テキスト中の接続詞等の手がかり語情報を基に，
文間の構造を解析することでテキスト構造を獲得し，
その構造を利用して重要文の獲得を試みることも考えられる．
接続詞，照応表現などの手がかりを用いたテキスト構造に基づき，
文に重要度を付与し要約を作成する．このテキスト構造の解析による手法の利点として，
テキスト構造に基づき重要文を抽出しているので，
単語の頻度などを用いた手法に比べて一貫性を保った要約を作成できる可能性がある．


\subsection{テキスト中の文間の類似性の利用}
情報検索の分野では，テキストやその断片をその中に出現する単語の重みの
ベクトルとして表現することが多い．
そのような表現方法をベクトル空間モデル（vector\ space\ model）\cite{vec_space}という．
いま重み付け対象となる文を$S_1,S_2,\ldots ,S_m$とし，
これらの文集合全体を通して全部で$n$個の内容語$w_1,,w_2,\ldots ,w_n$があるとする．
このとき，文$S_i$は，式(\ref{vector_space})のようなベクトルで表現されることになる．
\begin{equation}
\label{vector_space}
d_i=\left[
  \begin{array}{cccc}
     d_{i1}  &  d_{i2}  &  \ldots  &  d_{in}  \\
  \end{array}
\right]
\end{equation}
ここで，$d_{ij}$は内容語$w_j$の文$S_i$における重みである．
また，文書集合全体は，式(\ref{trem_matrix})のような$m\times n$行列$S$によって
表現することが出来る．
\begin{equation}
\label{trem_matrix}
S=\left[
  \begin{array}{c}
    d_{1}   \\ d_{2}   \\ \vdots   \\ d_{m}   \\
  \end{array}
\right]
=
\left[
  \begin{array}{cccc}
    d_{11}   & d_{12}   & \ldots  & d_{1n}   \\
    d_{21}   & d_{12}   & \ldots  & d_{1n}   \\
    \vdots   & \vdots   & \ddots  & \vdots   \\
    d_{m1}   & d_{m2}   & \ldots   & d_{mn}   \\
  \end{array}
\right]
\end{equation}

このベクトル空間モデルを応用した例として，ベクトルの内積で文間の結合度を計算し結合度の高い
文を抽出することが考えられる．

また，テキスト全体をあらわすベクトルと類似度が近い文ベクトルは重要であるとして
重要文を抽出する考えもある．
これはテキスト全体が表現する意味に，
近い意味を表現している文を要約文として採用していることと等価である．

そして，テキスト中の文を一単位としてそれらの間の類似度を計算し，
この類似度を文間のつながりの度合いと考え，この情報を基に重要と考えられる
文を抽出する方法がいくつか提案されている．具体的には，
文間のつながり度合いから，ある閾値以上の文同士をリンクとして結び，多くの文と
リンクで結ばれている文を複数の文に渡る主題について議論してるという考えを
抽出する方法である．

\subsection{文間，単語間の結びつきを利用}
テキスト中の文間のつながり情報を重要文抽出に利用する方法について述べる．
文をノード，文間の関係をリンクとするグラフをテキストから構成し，
多くの文と関係のある文が重要であるという考えに基づき，
重要文を抽出する手法が考えられている．
ここでは文中の単語が同一概念を参照しているような文間にリンクがあるとして作成したグラフを用いる．
表層的な文間のつながりを表す指標としては，
指示語，照応詞，省略，接続詞，語彙的結束性などが考えられる．

語彙的結束性は複数の文間に意味的なつながりを類義語や意味的に関連があると
連想される語彙が用いられることによって認めるものである．
この意味的つながりは語彙的連鎖と呼ばれ，各連鎖はテキスト中の話題を表すと考えられるため，
重要な連鎖を構成する単語を多く含む文が重要であるという重要文決定手法が
提案されている\cite{info_ret_sum} \cite{term_chain_cal}．

この中で，語彙的連鎖は次の手順で計算されている．まず，
コーパスを用いた単語の共起情報から二つの単語間の類似を式(\ref{coscr})
によって計算する．
\begin{equation}
\label{coscr}
coscr\left( X,Y\right)=
\frac{\sum_{i=1}^{n}x_{i}\cdot y_{i}}{\sqrt{\sum_{i=1}^{n}x_{i}^{2}}\cdot  \sqrt{\sum_{i=1}^{n}y_{i}^{2}}}
\end{equation}

ここで，$x_i$と$y_i$はテキスト$i$に単語$X$と単語$Y$が出現する数，要するに単語の頻度，
$n$はコーパスの全テキスト数を表す．次に，計算された類似度を基に，式(\ref{min_dis})
に示す最短距離法によって単語をクラスタリングする．
\begin{equation}
\label{min_dis}
sim\left( C_i, C_j\right)
=\displaystyle\max_{X\in C_i,Y\in C_l}coscr\left( X,Y\right)
\end{equation}

ここで，$X$，$Y$はクラスタ$C_i$内，$C_j$内の単語である．

ある閾値までクラスタをマージし，同一クラスタ内の単語によって語彙的連鎖を構成する．

要約の作成は単語の重要度に基づく場合と同様に，まず各連鎖の重要度$w_i$を計算し，
次に，式(\ref{weight_equation})により各文中の重要度の総和を計算し，
重要度$S_j$の高い文を要約率が要求する量だけ文を抽出する．
$w_i$を連鎖$i$の構成単語数$\left| i\right|$と連鎖$i$の出現テキスト数
$df_i$により式(\ref{term_chain})で計算される．
\begin{equation}
\label{term_chain}
w_i=\left| i\right|\times\log\frac{N}{df_i}
\end{equation}

ただし，$N$を全テキスト数とする．

この語彙的連鎖の重要度に基づく要約は要約システム$Posum$\cite{posum_1502}として実装されている．
そして，本研究の比較実験の要約器としても用いる．


\subsection{重要文抽出による要約の問題点}
重要文抽出による要約の利用目的や対象データにも依存するが，
次のような問題も指摘されている．
単語の出現頻度や，単語間のつながりの情報を用いた手法では，
テキストが複数の話題から構成されている場合，話題ごとに語彙の出現傾向が変わるため，
十分な精度得られない可能性がある．
同様に位置情報を用いた手法も，十分に機能しない場合も考えられる．このような場合には，テキストを何らかの手法で話題ごとに分割し，その話題ごとに重要文を抽出する必要がある．また重要文抽出のための情報のどれもが，全てのジャンルのテキストで有効に機能する保証はない．新聞記事や学術論文，物語文などテキストのジャンルが異なれば，有効な情報が異なるのは当然のことだと考えられる．重要度を決定する情報を統合して用いる場合でも，同様にして，テキストのジャンルによって，組み合わせ方で精度がばらつくことが報告されている．\cite{nomoto_2}

さらにテキスト中のいろいろな箇所から抽出したものを単に集めているため，抽出した複数の文間のつながり（守備一貫性）が悪いことが指摘されている．抽出した文中に指示語が含まれていても，その先行詞が要約中に存在しなかったり，また，不要な接続詞があったりする可能性がある．このような状況下では，読みにくいということはもちろんだが，最悪の場合，要約テキストの内容を誤読してしまう恐れがある．また，文を重要として集める際，他の文と独立に抽出をおこなっているので，結果として要約中に抽出された文の内容に類似のものがいくつも含まれるということが生じる可能性がある．このような，これまでの要約手法の問題点を受けて，「より読みやすい要約」，「より冗長性の少ない要約」を目指す動きが近年活発になってきている．
本研究もこの分野の研究の一部である．




\subsection{要約の評価}
\subsubsection{適合率と再現率}
これまでの要約研究の多くは，人間の被験者の生成した要約文と，システムの生成した要約文を比較し，システムの要約文の適合率と再現率を評価尺度とした評価を行っている．重要文抽出による要約の場合，テキスト中の文と要約集合内の文について，テキストに対する要約文の適合性が与えられたと仮定する．具体的には，被験者による要約を用意することによって，この仮定は満たされる．この時，ユーザの要約要求に対して，表\ref{kousa}のような交差行列を作ることができる．ここで，行は要約要求に対して適合するかどうかを表し，列は要約の対象になったかどうかを表している．行列の要素は文の数である．

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|c|c|c|} \hline
 & 要約された文書 & 要約されなかった文書 \\\hline
適合文 & w & x \\\hline
非適合文 & y & z \\\hline
\end{tabular}
\end{center}
\caption{交差行列}
\label{kousa}
\end{table}

表\ref{kousa}が与える適合率P(precison)と再現率R(recall)はそれぞれ次のように定義できる．
\begin{eqnarray}
適合率：P=\frac{w}{w+x}
\end{eqnarray}
\begin{eqnarray}
再現率：R=\frac{w}{w+y}
\end{eqnarray}

すなわち，適合率は要約された適合文（ｗ）と要約の対象となった文(ｗ＋ｙ)の比であり，要約にどれだけ「ゴミ」がないかを表している．
一方，再現率は要約された適合文数（ｗ）と要約対象となる文集合中の適合文書数（ｗ＋ｘ）の比であり，検索にどれだけ「漏れ」がないかを表している．
従って，適合率と再現率は大きい値をとるほど性能がよいことになる．

\subsubsection{その他の評価}
しかし，人間においても要約というタスクは必ずしも用意ではなく，
人間の被験者らによる要約が高い確率で一致するとは言えない．
また，評価において前提となっている「ただ一つの正しい要約が存在する」という仮定自体，
不自然であるという批判は以前からある．
よって何が適合文書なのかという定義は曖昧である．

この問題に対してこれまでいくつかの提案がなされている．
そのうち一つが文のutilityという概念を導入した評価方法である\cite{namba_02}．
文のutilityとは，そのテキストの話題に対する各文の適合度を10段階で表したものであり，正解の文のutilityにどのくらい近いutilityの文を選択できるかで評価を行う．
人間が選択した重要文を用いた今までの評価方法は，正解と一致した場合を１，一致しない場合を０として再現率，精度を計算していたが，
utilityに基づく評価値は，システムが選択した文に対して人間が割り当てたutilityの総和を正解の文のutilityの総和で割った値として計算する．
これまでの評価方法では，システムが選択した不正解の文は，全く評価が得られなかったのに対し，utilityに基づく部分的な評価が得られるという点が異なる．
ただ一つ解が存在し，それとまさに一致することを要求されていたこれまでの評価に比べ，
正解文のutilityにどれだけ近いutilityの文を選択できるかで評価を行う．

また，これに対して別の観点から，要約を利用して人間がタスクを行う場合のタスクの達成率が間接的に要約の評価とするという考えがある．
具体的には，情報検索における検索テキストの適切性の判断に，それぞれのテキストの要約を用いることで，要約を評価し，タスクに要する時間と検索の再現性，
適合率で評価を行っている．

また検索システムを用いたもう一つの例としては，抽出した要約文のみから索引付けした検索システムとフルテキストから索引付けした検索システムとを比較することによって
要約を評価する手法もある．
これは，フルテキストよりも要約文のみから索引付けした検索システムの精度が高いことを示すことによって，要約文が大意をつかみ，
テキストの重要な概念や単語を捉えていることを間接的に示している．



