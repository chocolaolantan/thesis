\chapter{大域ベクトル予測モデル}
\label{app_a}
\section{大域ベクトル予測モデル(Global Vector prediction, \textbf{GloVe})}
word2vecに実装されている予測モデルはどちらも一文単位で処理を繰り返し、意味表現を学習しているが、この大域ベクトル予測モデルでは、コーパス全体における二単語の共起情報から学習している。

word2vecでは交差エントロピー誤差の最小化をしており、学習に負例を要していたため、計算の高速化のためにいくつかの仮定やテクニックを要していたが、大域ベクトル予測モデルでは二乗誤差を目的関数としており、また負例を要しないため、仮定や近似を考える必要がない。

SVDなどにより共起行列の次元縮約を通じて意味表現を学習する手法では大きなコーパスの学習に時間がかかったり、出現頻度の少ない単語が大きな重要性を得てしまうことがあるなどする。また、word2vecなどの確率的な予測タスクから学習する手法では、大きなコーパスを必要とするが、単語類似度以外の応用にも用いることができる。

大域ベクトル予測モデルは、この2つの特徴を組み合わせ、利点をとる手法として提案されたもの\footnote{http://nlp.stanford.edu/projects/glove/}で、word2vecよりも少ないコーパスで、また、低次数な表現ベクトルで高い精度を見せているとのことである。

今回の研究に用いたWikipediaデータは更新することなどしなかったので、一括して処理する大域ベクトル予測モデルを利用しても良かったかもしれないが、今回はオンライン学習が可能なword2vecを利用した。
