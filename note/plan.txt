File name : plan.txt
Date      : 20160926

Title     : 卒業論文作成に向けて

【1. 卒業研究でやりたいこと】
  - ニューラルネットワーク(word2vec)の利用による単語ベクトルの取得
  - 取得した単語ベクトルを用いた、なんらかの自然言語処理
    → 文書要約
　
【2. 研究目的】
  未定。
  やりたいことから導かれる、何らかの自然言語処理。
  - 候補
    * 文書要約          ← ◎
    * 会話ボットの作成

  ※会話ボットの作成は評価尺度の定義が難しい。
    文書要約なら圧縮率などで定義可能。また、先行研究例あり。

  - 文書要約
      |- 抽出型
      |  |
      |  |- 重要文抽出  : テキストから重要な文を抜き出す。   ← 先行研究あり
      |  |
      |  |- 文短縮     : 各文を短縮する
      |
      |- 生成型    : 抽出型以外の手法。言い換えなどの表現を駆使する。

【3. 自然言語処理】
  1.) ことば、とは
    - 人と人とのコミュニケーションに用いられる道具。
    - 文字、あるいは音の並びによって構成されている。
    - 文字の並びによって構成される単語、の並びによって文が構築される。
    ※最小単位として、形態素を用いることにする。
      * 形態素 ： 文字列が意味、或いは役割をもつ最小のまとまり

  2.) 言語モデル、とは
    - 日本語らしさを判定するものさしのようなもの
    - 多くの言語モデルでは確率が用いられる。

    らしさ。は明確にわけられるものではなく、また、現実問題として判別を間違うことも起こりうるが、正解、不正解を１，０であらわしたとして、間違えてしまった時の影響が大きくなる。そのため度合いであらわされることが望ましい。
    度合いが表現できるならなんでもいいわけだが、確率を用いるのは、0-1でわかりやすく、扱いやすいということに加え、確率を用いると、ベイズの公式など種々の確率論により、応用が広がるという利点がある。
    例) 音響モデル


【4. 機械学習】
  コンピュータが計算処理によって学習すること。
  → 「データの集合」から、「そこにある法則性」を学ぶこと。

  一度与えられたデータから、汎用的な法則性を学び、将来予測や未知データの推定を行う

  1.) データ分析における目的例
    - 回帰        : 入力と出力の因果関係を調べる
    - 分類        : カテゴリを予測する
    - クラスタリング : グループを見つける

  2.) 機械学習のステップ
    - 生データから特徴ベクトルの抽出
      観測した生データを、法則性を導き出すために使いじｄやすい形式に変換する

    - 機械学習アルゴリズムの適用
      どう定式化し、どのようなアルゴリズムを適用するかを決めて、特徴ベクトルを入力する

  3.) 機械学習の分類
    - 教師あり
      * 回帰          : 予測対象が数値
      * 分類          : 予測対象がカテゴリ
    - 教師なし
      * クラスタリング   : グループを見つけ出す
      * 次元削減      : 多変数のデータを要約する
    - 応用例
      * 推薦          : 膨大なアイテムからユーザーに適したものを提示する
      * 異常検知      : データの中から明らかに他とは異なるサンプルを見つける


【5. 単語ベクトルの取得】
  機械学習を用いて大量のデータを処理。
  教師なし。教師ありができるほど文法などを習熟していない。

【6. 自動要約】
  1.) 手法
    - 館林さんのクリークを見つけて抽出
    - 館林さんの修論に、word2vecによって取得した単語ベクトルの利用
    - 一つの文書を入力とし、文書ベクトルを作成（次元圧縮）。このベクトルから文書を再構築。これが要約になっているのでは
    - LSTM（Long Short Term Memory）ネットワークの利用。  ← ジェイさんにいただいた提案？

【?. LSTM(Long Short Term Memory)ネットワーク】
  - 人間の思考は持続性を持っている。
  - 従来のニューラルネットワークではこれを再現できない
   → そこで、”リカレント ニューラルネットワーク”

  1.) リカレント ニューラルネットワーク（RNN）
    - 内部にループ構造を持ち、情報を持続させることができる。
    - 以前の情報を、後のタスクに関連付けることができる。
      * 関連する情報と、それを必要とする情報のギャップが大きくなると、学習が難しくなる。
      （長期の依存性）
    - すべてのRNNは、ニューラルネットワークのモジュールを繰り返す、鎖状である。
    - 標準のRNNでは、この繰り返しモジュールに単一のtanh層を持つ。

  2.) LSTMネットワーク
    - 長期の依存性を学習できるように明示的に設計されている。
    - 繰り返しモジュールは、単一のニューラルネットワーク層ではなく、４つの層を持つ。

      * 通常のRNNや深層学習においては、誤差逆伝播法における誤差信号が意図した大きさで伝播しない、勾配消失（爆発）問題が生じる。
        ← 活性化関数の微係数を何度も乗算されることで生じる。

    - 入力、忘却、出力の３つのゲートを持つ
      * 入力  : 前時刻からの加算処理を行うか否か
      * 忘却  : 隣接層からの入力の加算を行うか否か
      * 出力  : 出力を行うか否か
      現在・直前のメモリーノードの値や隣接層からの入力ベクトルをもとに判断する。

    ◎LSMTの基本動作ステップ
      ⅰ. 過去の状態と出力h_t-1を引き継ぐ
      ⅱ. 忘却ゲート
        時刻t-1の出力と時刻tの入力を結合して一つのベクトルにし、行列をかけてバイアスを加えた後にシグモイドに通して、時刻t-1の状態と要素ごとの積をとる。忘却ゲートの結果が０なら状態の要素は忘れて、１なら完全に覚えている。ということになる。
      ⅲ. 入力ゲート
        tanhのゲートで次の状態の候補を算出する。算出した候補と入力ゲートの結果を要素ごとに掛け合わせ、ステップⅱで忘却処理した結果と足し合わせることで時刻tの状態を計算する。
      ⅳ. 出力ゲート
        ステップⅱの結果と、時刻tの状態をtanhに通したものとの要素ごとの積をとったものを、時刻tの出力とする。
